# Interpolate Qualified

This tool is a preliminary attempt to better deal with the problem of qualified,
or truncated values in a QSAR dataset.

The typical case will be an assay for which the sensitivity of the assay may have
changed over time. For example, early measurements might have had nothing
below 1 MicroMol, but subsequent assay development may now allow measured
values lower than that. So the data will consist of some values like `<1`
and others like `0.2`. We have no information about how different from 1.0
that '<1`' value is. Might be 0.95, might be 0.01...

This tool uses the rest of the dataset to build models on the data and to
predict the qualified values. If a prediction is less than 1.0, that prediction
is subsequently used as the "experimental" value in model building. If no predicted value
is below 1.0, that value is retained. Similarly for cases where experimental
values are truncated at the upper end, `>10`.

The tool works by first separating qualified measurements from non-qualified
values. An XGBoost model is built on the non-qualified values and that 
model predicts the qualified values. Each qualified measurement keeps track
of the predictions made for it, and at any time can report its
"best estimate" of the true activity - either a predicted value or the
starting, truncated, value.

Then batches of qualified values are added to the unqualified set, using
their "best estimate" of activity, and the remaining qualified values
predicted. Again, predicted values are recorded.

In the final pass, all molecules for which there has been a successful
extrapolation, as well as unqualified measurements, are used to predict
the last qualified values - those for which no model has predicted beyond
the truncated value.

A new activity file is written, which can be used in model building.

## Discussion
This is a first pass on this concept, and I think it needs to be refined.
If there are a small number of qualified values, then optimisation strategies
might not much matter.

But it is easy to imagine that the order in which qualified values are
processed could influence the outcome. If qualified result A is "close" to qualified result B, but not
close to anything else, a prediction of A may always be within the qualified
range. But if B, which might be closer to unqualified values, is added first,
then A might be predicted outside the truncated range.

The `-rand` option, samples from the qualified values randomly, rather than
sequentially. It is unclear whether this is beneficial or not - early tests
are inconclusive.

Consider this an alpha version, subject to change and hopefully improvement.

Solving the problem exhaustively is likely infeasible.

## Performance
Models are built and scored using the C interface to XGBoost, which is fast. An
8k test set I used, can build and score the unqualified vs qualified dataset in
about 2 seconds - running 8 way parallel. An earlier idea was to build this
app as a cluster based app, which could then send various model combinations to
a cluster. But then the collating task becomes much more complex.

If you have a large dataset, suggest trimming down to those unqualified molecules that are
"close" to the qualified measurements, and process that subset. Those molecules
should be the ones that exert most influence on the predicted values for
the qualified measurements.

## HOWTO
The tool required an activity file and a file of molecular descriptors to use
for model building. `iwdescr.sh` seems to work well, but any set of descriptors
that can build a reasonable model should be fine.

Both files need to be space separated, both with header records.

The activity file must contain qualified data in the form '<y' or '>y'. So
a valid input activity file might look like
```
ID Activity
ID1 1.1
ID2 <2.0
ID3 >4.0
ID4 5.0
```
and a valid descriptor file will be what is generated by iwdescr.sh.

Within the tool there is the concept of a batch of qualified measurements that
are added to the known set and used to build a model. The fewer measurements there
are in each batch, the more models will need to be built and scored, which increases
run-times. But having a small batch size is expected to increase the tool's
ability to find extrapolated predictions.

A typical usage might be

```
iwdescr.sh file.smi > file.w

interpolate_qualified -X /tmp/file.w -Y /tmp/file.activity -batch 10 -v -initial -rand
```

The -X option specifies the features used for model building, while the -Y option
specifies the response - including qualified activities.

The result will be a file 'intplq.dat' which is a new activity file, containing
extrapolated values where possible. Because the `-initial` option is given,
that file will contain an extra column with the initial values. The name of
the file created can be changed via the -S option.
